%        File: draft.tex
%     Created: 四 11月 21 02:00 下午 2019 C
% Last Change: 四 11月 21 02:00 下午 2019 C
%
\documentclass{utils/mydoc}
\usepackage{amsmath}

\title{\bfseries \sffamily Paper Notes}
\author{Bingb Hu \\ @SIST, ShanghaiTech University}
\date{\today}

\begin{document}
\maketitle

Machie learning and deep learning training sets are preferred to be uniformly
distributed, but in real world, imbalance exists everywhere. If we can get a
good performance in imbalanced data, isn't it very nice?

\begin{itemize}
  \item InfoGAN maximize the mutual information between $c$ and $G(z,c)$.
    Is it possible to replace MI into conditional entropy, i.e., 
    $H(c|\tilde{x})$?
\end{itemize}

The objective of EWGAN is generating samples for minority class, by learning
the data distribution from the given dataset.

About eval metric, as InfoGAN says, $H(c|\tilde{x})$ should be low, thus
average entropy can be chosen as a metric.

\section{Elastic-infoGAN}

\begin{itemize}
  \item Learns to disentangle object identity from other low-level aspects in 
    class-imbalanced datasets.
  \item  better disentanglement of object identity
  \item  better approximation of class imbalance in the data
\end{itemize}

\end{document}


